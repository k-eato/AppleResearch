# Apple Detection and Classification
This project was developed to train a neural network to locate apples in images, as well as a second neural network to classify them between healthy and rotten apples. The targetted application is to help food processing plants quickly and accurately sort produce, specifically for removing rotten food. While this can be done with humans, computers could offer a faster and cheaper alternative. Computer based systems do currently exist for this already but from what was found, they rely on computer vision whereas a neural network based approach is proposed here. In order to do this, TensorFlow and its Object Detection API need to be installed and utilized. The following sections walk through the various aspects of the project leading to the working solution. The goal is to demonstrate the work accomplished and also explain how to do it for those curious about working with the TensorFlow tools on a similar project.

## Datasets
Over the course of the project's work, three different datasets were gathered for each portion of the project, leading to six in total. 

#### Object Detection
For the object detection prtion, the first one captured consists of pictures of actual apples from an organic farm. These apples had lots of defects from brown spots to mold, helping to gather images of rotten apples since these would be the harder to find than images of healthy apples. Most of the pictures are various angles of a single apple against a blank piece of paper. However, a couple images at the end include multiple apples with other objects to provide a greater challenge to the neural network. Some samples from the data are shown below, while the rest can be found following the directory structure "Object Detection/Data/Actual Photos"<br/>
![Detection Dataset 1](https://github.com/k-eato/AppleResearch/blob/master/Object%20Detection/Data/Actual%20Photos/0.jpg)
![Detection Dataset 1](https://github.com/k-eato/AppleResearch/blob/master/Object%20Detection/Data/Actual%20Photos/101.jpg)<br/>

The issue with this approach is the difficulty in creating a large enough dataset, so the other two datasets were created with augmented images. A single Python script (dc.py) was used to create both by changing the inputs. It randomly places a number of apples on a backdrop meant to look like a conveyor belt due to the project goal of helping food processing plants. One dataset is meant for training while the other is for testing the model afterwords. The script also creates XML files for each new image, since they are needed later to tell where the apples are located in each. Pictures from each are shown below.<br/>
![Detection Dataset 2](https://github.com/k-eato/AppleResearch/blob/master/Object%20Detection/Data/Training/200.jpg)
![Detection Dataset 3](https://github.com/k-eato/AppleResearch/blob/master/Object%20Detection/Data/Testing/19.jpg)<br/>

#### Classification
The three datsets used with the classification model are split for the purposes of training, validation, and testing. The training dataset features images of healthy and rotten apples against the background of the belt from the training dataset of the Object Detection portion. Likewise, the testing dataset uses the blue belt background of the object detection test dataset. All of the images used for this come from data of rotten and fresh fruits found on [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification). For the validation dataset, the images are used as found in this dataset to hopefully provide better contrast to the train and test datasets in order to fight overfitting. To increase the size of the datasets, two Python scripts (rotate.py and brighten.py) are used to rotate and affect the brightness of all the images passed to them. This effectively creates 11 new images from the original. Sample results of this are shown below.<br/>
![Classification Data](https://github.com/k-eato/AppleResearch/blob/master/Classification/Data/augmentation_results.JPG)<br/>

## Object Detection Pipeline
Once the Dataset has been acquired, the next step is to accumulate all the XML files for the images into a CSV file. This is done by running the script "xml_to_csv.py" for each set of images separately (training and testing). The CSV file resulting from this is then used to create a binary record file in a data format the TensorFlow system can understand. In this case, the Pascal format is used in the file "create_pascal_tf_record.py" to make the record file. A pbtxt file is also needed to specify the different classes of objects to detect. These should match the labels given in the XML files. The format of this can be seen in the "labels.pbtxt" file provided in the directory. The last file that is necessary to modify is a config file, which will come from whichever pretrained neural network you choose to use. I used a SSD MobileNetV1 model, which can simply be downloaded from the provided [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md). A pretrained model is necessary to perform transfer learning, which is a better approach in this case than training a model from scratch because it limits the dataset size and training time necessary to achieve sufficient performance. After roughly an hour and a half of training the model, il was able to detect amost all of the apples in the test dataset. A couple of the results are shown below.<br/>
<img src="https://github.com/k-eato/AppleResearch/blob/master/Object%20Detection/Results/result21.PNG" width=400 height=225 align=left>
<img src="https://github.com/k-eato/AppleResearch/blob/master/Object%20Detection/Results/result22.PNG" width=400 height=225 align=left> 
 
 <br/><br/><br/><br/><br/><br/><br/><br/><br/>

## Classification Pipeline
The classification portion of the project was simpler and could be condensed into a single Python file, classify.py. Once the datasets are assembled, they can be read into the program with three ImageDataGenerator objects, for training, validating, and testing. The next portion of code I added defines a function for displaying data images with labels. This is unecessary but helpful for visulizing results and the input image and classification. Next, the desired pretrained neural network should be imported from Keras Applications. I chose the VGG16 model because it performed the best by far, although I was hoping a smaller, faster one would work equally well. Overall, itthe VGG16 model was able to classify 96% of the test images correctly. Once the model is imported, it is necessary to define additional fully connected layers, since these are not imported (include_top is set to false). The training parameters are then set and training is performed, followed by testing. The testing is done by iterating through batches of the test data and appears to be infinite so I just stopped the script once I had a satisfactory number of testing accuracies to calculate a mean accuracy from. Below is a visual from one batch of testing results.<br/>
<img src="https://github.com/k-eato/AppleResearch/blob/master/Classification/ClassificationResult.PNG" width=900 align=left><br/>
